{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An implementation of Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import math\n",
    "import time\n",
    "\n",
    "import gmpy2\n",
    "from gmpy2 import mpz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset\n",
    "df = pd.DataFrame(pd.read_csv('../data/compas-binary.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = df.as_matrix()[:,:13]\n",
    "\n",
    "y = df.as_matrix()[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association Rule Mining (Only one feature)\n",
    "\n",
    "#support\n",
    "#supp = [(x[:,i]*y).mean() for i in range(13)]\n",
    "#supp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confidence\n",
    "#conf1 = [sum(x_all[:,i]*y)/sum(x_all[:,i]) for i in range(13)]\n",
    "#conf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confidence\n",
    "#conf0 = [sum((x_all[:,i]==0)*y)/sum((x_all[:,i]==0)) for i in range(13)]\n",
    "#conf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBecause Using both conf1 and conf0 would select out too many features, \\nwhich is hard for the algorithm to run out,\\njust use conf1 to select out a small fraction of feature.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_idx = [conf1[i]>0.5 or conf0[i]>0.5 for i in range(len(conf1))]\n",
    "\"\"\"\n",
    "Because Using both conf1 and conf0 would select out too many features, \n",
    "which is hard for the algorithm to run out,\n",
    "just use conf1 to select out a small fraction of feature.\n",
    "\"\"\"\n",
    "#x_idx = [conf1[i]>0.5 for i in range(len(conf1))]\n",
    "#x_idx[0] = True # in the CORELS paper, gender is an important feature, so I add it manually\n",
    "#x_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select out these features\n",
    "#x = x_all[:,x_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#manaually select out 5 features, accoring to CORELS paper when lambda=0.01\n",
    "# sex:Female, age:18-20,age:21-22, juvenile-crimes:=0, priors:>3\n",
    "x_idx = [0,1,2,8,12]\n",
    "x = x_all[:,x_idx]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#manaually select out 6 features, accoring to CORELS paper when lambda=0.01\n",
    "# sex:Female, age:18-20,age:21-22, juvenile-crimes:=0, priors:2-3, priors:>3\n",
    "x_idx6 = [0,1,2,8,9,12]\n",
    "x6 = x_all[:,x_idx6]\n",
    "x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrule = x.shape[1]\n",
    "ndata = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following functions are copied from rule.py in bbcache ###\n",
    "\n",
    "\"\"\"\n",
    "    Python implementation of make_default\n",
    "\n",
    "    Returns a mpz object consisting of length ones\n",
    "\n",
    "    Note: in order to ensure you have a leading one, pass in\n",
    "    a length that is 1 greater than your number of samples\n",
    "\"\"\"\n",
    "def make_all_ones(length):\n",
    "    ones = pow(2, length) - 1\n",
    "    default_tt = mpz(ones)\n",
    "    return default_tt\n",
    "\n",
    "\"\"\"\n",
    "    Python implementation of rule_vand\n",
    "\n",
    "    Takes in two truthtables\n",
    "    Returns the and of the truthtables \n",
    "    as well as the number of ones in the and\n",
    "\"\"\"\n",
    "def rule_vand(tt1, tt2):\n",
    "    vand = tt1 & tt2\n",
    "    # subtract 1 to remove leading ones\n",
    "    cnt = gmpy2.popcount(vand) - 1\n",
    "    return vand, cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Python implementation of make_default\n",
    "\n",
    "    Convert a binary vector to a mpz object\n",
    "\n",
    "    Note: in order to ensure you have a leading one,\n",
    "    add '1' in the front\n",
    "\"\"\"\n",
    "def rule_vectompz(vec):\n",
    "    return mpz('1'+re.sub('[\\[\\],\\s+]','',str(list(vec))),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate z, which is for the equivalent points bound\n",
    "z is the vector defined in algorithm 5 of the CORELS paper\n",
    "z is a binary vector indicating the data with a minority lable in its equivalent set\n",
    "\"\"\"\n",
    "z = pd.DataFrame([-1]*ndata).as_matrix()\n",
    "# enumerate through theses samples\n",
    "for i in range(ndata):\n",
    "    #if z[i,0]==-1, this sample i has not been put into its equivalent set\n",
    "    if z[i,0] == -1:\n",
    "        tag1 = np.array([True]*ndata)\n",
    "        for j in range(nrule):\n",
    "            rule_label = x[i][j]\n",
    "            #tag1 indicates which samples have exactly the same features with sample i\n",
    "            tag1 = (x[:,j] == rule_label)*tag1\n",
    "            \n",
    "        y_l = y[tag1]\n",
    "        pred = int(y_l.sum()/len(y_l) > 0.5)\n",
    "        #tag2 indicates the samples in a equiv set which have the minority label\n",
    "        tag2 = (y_l != pred)\n",
    "        z[tag1,0] = tag2\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheTree:\n",
    "    \"\"\"\n",
    "    A tree data structure.\n",
    "    prefix: a 2-d tuple to encode the leaves\n",
    "    num_captured: a list to record number of data captured by the leaves\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, prefix,\n",
    "                 lamb, prior_metric = None,\n",
    "                 num_captured=None,\n",
    "                 deadleaf = None, \n",
    "                 splitleaf = None,\n",
    "                 lbound=None,\n",
    "                 p = None, \n",
    "                 B0 = None, points_cap = None):\n",
    "        self.prefix = prefix\n",
    "        #self.prediction = prediction\n",
    "        self.num_captured = num_captured\n",
    "        #self.num_captured_incorrect = num_captured_incorrect\n",
    "        self.p = p # the proportion of misclassified data in each leaf\n",
    "        self.deadleaf = deadleaf #a list indicate which leaves will never be split (support bound)\n",
    "        self.splitleaf = splitleaf #a queue of lists indicating which leaves will be split in next rounds (1 for split, 0 for not split)\n",
    "        self.lbound = lbound #a list of lower bound\n",
    "        self.B0 = B0 # a list of b0\n",
    "        self.points_cap = points_cap #a list of mpz, indicating which data are captured by each leaf\n",
    "        \n",
    "        ndata = len(y)\n",
    "        l = len(prefix)\n",
    "        \n",
    "        self.risk = self.lbound[0]+(self.p[0]*self.num_captured[0])/ndata\n",
    "        \n",
    "        #print(self.prefix)\n",
    "        #print(self.lbound)\n",
    "        #print(self.splitleaf)\n",
    "        # which metrics to use for the priority queue\n",
    "        if (self.num_captured[0]==ndata):\n",
    "            # this case is when constructing the null tree ((),)\n",
    "            self.metric = 0\n",
    "        elif (prior_metric==\"curiosity\"):\n",
    "            self.metric = min([self.lbound[i]/((ndata-self.num_captured[i])/len(y)) \n",
    "                                 if self.splitleaf[0][i]==1 else float('Inf') for i in range(l)])\n",
    "        elif (prior_metric==\"bound\"):\n",
    "            self.metric = min([self.lbound[i] if self.splitleaf[0][i]==1 else float('Inf') for i in range(l)])\n",
    "        elif (prior_metric==\"entropy\"): \n",
    "            # entropy weighted by number of points captured\n",
    "            self.entropy = [(-self.p[i]*math.log2(self.p[i])-(1-self.p[i])*math.log2(1-self.p[i]))*self.num_captured[i] \n",
    "                            if self.p[i]!=0 and self.p[i]!=1 else 0 for i in range(l)]\n",
    "            self.metric = min([sum(self.entropy[:i]+self.entropy[i+1:])/(ndata-self.num_captured[i]) \n",
    "                               if (ndata-self.num_captured[i])!=0 else 0 for i in range(l)])\n",
    "        elif (prior_metric==\"gini\"):\n",
    "            # gini index weighted by number of points captured\n",
    "            self.giniindex = [(2*self.p[i]*(1-self.p[i]))*self.num_captured[i] for i in range(l)]\n",
    "            self.metric = min([sum(self.giniindex[:i]+self.giniindex[i+1:])/(ndata-self.num_captured[i]) \n",
    "                               if (ndata-self.num_captured[i])!=0 else 0 for i in range(l)])\n",
    "        else:\n",
    "            self.metric = 0\n",
    "\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        # define <, which will be used in the priority queue\n",
    "        return self.metric<other.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache every leaf\n",
    "\n",
    "class CacheLeaf:\n",
    "    \"\"\"\n",
    "    A data structure to cache every single leaf (symmetry aware)\n",
    "    \"\"\"\n",
    "    def __init__(self, antecedent, x, y, parent_points_cap):\n",
    "        \n",
    "        tag = parent_points_cap # points captured by the leaf's parent leaf\n",
    "        rule_index = abs(antecedent[0][-1])-1 #the leaf's last feature\n",
    "        rule_label = int(antecedent[0][-1]>0) #this binary feature is 0 or 1\n",
    "        #print(\"np.array(x[:,rule_index] == rule_label)\",x[:,rule_index] == rule_label)\n",
    "        tag_rule = rule_vectompz(np.array(x[:,rule_index] == rule_label)*1)\n",
    "        tag, self.num_captured = rule_vand(tag, tag_rule)\n",
    "\n",
    "        self.points_cap = tag\n",
    "\n",
    "        # the y's of these data captured by leaf antecedent[0]\n",
    "        #y_leaf = y[tag]\n",
    "        #print(\"tag\",tag)\n",
    "        #print(\"y\",y)\n",
    "        _, num_ones = rule_vand(tag,rule_vectompz(y))\n",
    "\n",
    "        #b0 is defined in (28)\n",
    "        \n",
    "        \n",
    "        tag_z = rule_vectompz(z.reshape(1,-1)[0])\n",
    "        _, num_errors = rule_vand(tag, tag_z)\n",
    "        self.B0 = num_errors/ndata\n",
    "\n",
    "        if self.num_captured:\n",
    "            self.prediction = int(num_ones/self.num_captured > 0.5)\n",
    "            if self.prediction == 1:\n",
    "                self.num_captured_incorrect = self.num_captured-num_ones\n",
    "            else:\n",
    "                self.num_captured_incorrect = num_ones\n",
    "            self.p = self.num_captured_incorrect/self.num_captured\n",
    "        else:\n",
    "            self.prediction = 0\n",
    "            self.num_captured_incorrect = 0\n",
    "            self.p = 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eliminate:\n",
    "    \"\"\"\n",
    "    A data structure to record and identify\n",
    "    whether a tree has been visited/pruned\n",
    "    \"\"\"\n",
    "    def __init__(self, elim_dict = None):\n",
    "        self.elim_dict = {} # record these trees we have visited\n",
    "        \n",
    "    def eliminate(self, prefix):\n",
    "        self.elim_dict[tuple(sorted(prefix))] = 1\n",
    "        \n",
    "    def is_duplicated(self, prefix):\n",
    "        # if a tree is in the self.elim_dict, then we have already visited it\n",
    "        return tuple(sorted(prefix)) in self.elim_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(lines, lamb, tic, queue_size, prefix_old, tree_new, R, d_c, R_c):\n",
    "    \"log\"\n",
    "    t = tree_new.prefix\n",
    "    t_c = d_c.prefix\n",
    "    \n",
    "    the_time = str(time.time()-tic)\n",
    "    the_queue_size = str(queue_size)\n",
    "    the_split_tree = str(prefix_old)\n",
    "    the_new_tree = str(t)\n",
    "    the_new_tree_length = str(len(t))\n",
    "    the_new_tree_objective = str(R)\n",
    "    the_best_tree = str(t_c)\n",
    "    the_length = str(len(t_c))\n",
    "    the_obj = str(R_c)\n",
    "    the_lbound = str(d_c.lbound)\n",
    "    the_accuracy = str(1-(R_c - lamb*len(t_c)))\n",
    "    the_num_cap = str(d_c.num_captured)\n",
    "\n",
    "\n",
    "    line = \";\".join([the_time, the_queue_size, the_split_tree, \n",
    "                     the_new_tree, the_new_tree_length, the_new_tree_objective,\n",
    "                     the_best_tree, the_length, the_obj, \n",
    "                     the_lbound, the_accuracy, the_num_cap])\n",
    "    lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_splitleaf(splitleaf_list, cap_l, incorr_l, ndata, t, lb, b0, lamb, R_c):\n",
    "    \"\"\"\n",
    "    generate the new splitleaf for the new tree\n",
    "    \"\"\"\n",
    "    splitleaf_array = np.array(splitleaf_list)\n",
    "    sl = splitleaf_list.copy()\n",
    "\n",
    "    #(Lower bound on accurate antecedent support)\n",
    "    a_l = (sum(cap_l)-sum(incorr_l))/ndata - sum(cap_l)/ndata/2\n",
    "\n",
    "    #binary vector indicating split or not\n",
    "    splitleaf1 = [1]*(len(t)) #all leaves labeled as to be split\n",
    "    splitleaf2 = [0]*(len(t)-2)+[1,1] #l1,l2 labeled as to be split\n",
    "    splitleaf3 = [1]*(len(t)-2)+[0,0] #dp labeled as to be split\n",
    "\n",
    "    if lb+b0+lamb>=R_c or lb>=R_c:\n",
    "        #print(\"lb+b0+lamb\",lb+b0+lamb)\n",
    "        #print(\"R_c\",R_c)\n",
    "        # if equivalent points bound combined with the lookahead bound doesn't hold\n",
    "        # or if the hierarchical objective lower bound doesn't hold\n",
    "        # we need to split at least one leaf in dp\n",
    "\n",
    "        if a_l < lamb:\n",
    "        # if the bound doesn't hold, we need to split the leaf l1/l2 further\n",
    "\n",
    "            if len(splitleaf_list)>0:\n",
    "                split_l1_l2 = splitleaf_array[:,-1].sum()+splitleaf_array[:,-2].sum()\n",
    "\n",
    "                # if dp will have been split\n",
    "                if splitleaf_array.sum()-split_l1_l2>0:\n",
    "\n",
    "                    # if l1/l2 will have been split\n",
    "                    if split_l1_l2>0:\n",
    "                        sl.append(splitleaf1)\n",
    "\n",
    "                    # if l1/l2 will not have been split, we need to split l1/l2\n",
    "                    else:\n",
    "                        sl.append(splitleaf2)\n",
    "\n",
    "                # and we need to split leaves in dp, if dp will not have been split\n",
    "                else:\n",
    "\n",
    "                    # if l1/l2 will have been split\n",
    "                    if split_l1_l2>0:\n",
    "                        sl.append(splitleaf3)\n",
    "\n",
    "                    # if l1/l2 will not have been split, we need to split l1/l2\n",
    "                    else:\n",
    "                        sl.append(splitleaf2)\n",
    "                        sl.append(splitleaf3)\n",
    "            else:\n",
    "                sl.append(splitleaf2)\n",
    "                sl.append(splitleaf3)\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            if len(splitleaf_list)>0:\n",
    "                split_l1_l2 = splitleaf_array[:,-1].sum()+splitleaf_array[:,-2].sum()\n",
    "\n",
    "                # if dp will have been split\n",
    "                if splitleaf_array.sum()-split_l1_l2>0:\n",
    "                    sl.append(splitleaf1)\n",
    "\n",
    "                # and we need to split leaves in dp, if dp will not have been split\n",
    "                else:\n",
    "                    sl.append(splitleaf3)\n",
    "            else:\n",
    "                sl.append(splitleaf3)\n",
    "    else:\n",
    "\n",
    "        if a_l < lamb:\n",
    "            # if the bound doesn't hold, we need to split the leaf l1/l2 further\n",
    "\n",
    "\n",
    "            if len(splitleaf_list)>0:\n",
    "                split_l1_l2 = splitleaf_array[:,-1].sum()+splitleaf_array[:,-2].sum()\n",
    "\n",
    "                # if l1/l2 will have been split\n",
    "                if split_l1_l2>0:\n",
    "                    sl.append(splitleaf1)\n",
    "\n",
    "                # if l1/l2 will not have been split, we need to split l1/l2\n",
    "                else:\n",
    "                    sl.append(splitleaf2)\n",
    "            else:\n",
    "                sl.append(splitleaf2)\n",
    "\n",
    "        else:\n",
    "            sl.append(splitleaf1)\n",
    "        \n",
    "    return sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbound(x, y, lamb, prior_metric = None, MAXDEPTH = 4, niter=float('Inf')):#\n",
    "    \"\"\"\n",
    "    An implementation of Algorithm\n",
    "    ## one copy of tree\n",
    "    ## mark which leaves to be split\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize best rule list and objective\n",
    "    #d_c = None\n",
    "    #R_c = 1\n",
    "\n",
    "    nrule = x.shape[1]\n",
    "    ndata = len(y)\n",
    "    print(\"nrule:\", nrule)\n",
    "    print(\"ndata:\", ndata)\n",
    "    \n",
    "    E = Eliminate()\n",
    "    tic = time.time()\n",
    "    \n",
    "    lines = [] # a list for log\n",
    "    leaves = {} # cache leaves\n",
    "\n",
    "    # initialize the queue to include just empty root\n",
    "    queue = []\n",
    "    t = ((),)    \n",
    "    tree0 = CacheTree(prefix = t, x = x, y = y, lamb=lamb, prior_metric=prior_metric, \n",
    "                      num_captured=[ndata], deadleaf = [0], splitleaf = [[1]], lbound=[lamb],\n",
    "                      p = [min(np.mean(y),1-np.mean(y))], B0 = [np.sum(z)/ndata], points_cap = [make_all_ones(ndata+1)])\n",
    "    heapq.heappush(queue, (tree0.metric,tree0))\n",
    "    #queue.append(tree0)\n",
    "    d_c = tree0\n",
    "    R_c = tree0.risk\n",
    "    #log(lines, lamb, tic, len(queue), tuple(), tree0, R, d_c, R_c) \n",
    "    \n",
    "    COUNT = 0 #count the total number of trees in the queue\n",
    "    while (queue) and COUNT<niter:\n",
    "        #tree = queue.pop(0)\n",
    "        (curio, tree)=heapq.heappop(queue)\n",
    "        d = tree.prefix\n",
    "        \n",
    "        \n",
    "        #print(\"=======COUNT=======\",COUNT)\n",
    "        #print(\"d\",d)\n",
    "        #print(\"R\",tree.lbound[0]+(tree.num_captured_incorrect[0])/len(y))\n",
    "        \n",
    "        # if we have visited this tree\n",
    "        if E.is_duplicated(d):\n",
    "            continue\n",
    "        else:\n",
    "            E.eliminate(d)\n",
    "        \n",
    "        # the leaves we are going to split\n",
    "        split_next = tree.splitleaf.copy()\n",
    "        spl = split_next.pop(0)\n",
    "        \n",
    "        # enumerate through all the leaves\n",
    "        for i in range(len(d)):\n",
    "            #print(\"d!!!\",d)\n",
    "            # if the leaf is dead, then continue\n",
    "            if tree.deadleaf[i]==1:\n",
    "                continue\n",
    "            \n",
    "            #(Lower bound on antecedent support)\n",
    "            # if this bound doesnot hold, set the leaf to be dead, and continue\n",
    "            if tree.num_captured[i]/ndata/2 < lamb:\n",
    "                tree.deadleaf[i] = 1\n",
    "                continue\n",
    "            \n",
    "            # 0 for not split; 1 for split\n",
    "            #if spl[0][i]==0:\n",
    "            if spl[i]==0:\n",
    "                continue\n",
    "\n",
    "            d0 = d[i] #d0 is the leaf we are going to split\n",
    "            dp = d[:i]+d[i+1:] #dp is the rest\n",
    "            \n",
    "            \n",
    "            # Restrict the depth of the tree\n",
    "            if len(d0)>=MAXDEPTH:\n",
    "                continue\n",
    "            \n",
    "            # we are going to split leaf i, and get 2 new leaves\n",
    "            # we will add the two new leaves to the end of the list\n",
    "            splitleaf_list = [split_next[k][:i]+split_next[k][i+1:]+split_next[k][i:i+1]*2\n",
    "                              for k in range(len(split_next))]\n",
    "            \n",
    "            \n",
    "            lb = tree.lbound[i] # the lower bound \n",
    "            #print(\"tree.B0\",tree.B0)\n",
    "            b0 = tree.B0[i] # the b0 defined in (28) of the paper\n",
    "            \n",
    "            \n",
    "            \n",
    "            # split the leaf d0 with feature j\n",
    "            for j in range(1, nrule+1):\n",
    "                if (j not in d0)and(-j not in d0):\n",
    "                    # split leaf d0 with feature j, and get 2 leaves l1 and l2\n",
    "                    l1 = d0+(-j,)\n",
    "                    l2 = d0+(j,)\n",
    "                    t = dp+(l1, l2) # t is the new tree\n",
    "                    #print(\"t\",t)\n",
    "\n",
    "                    # if tree t is duplicated, continue\n",
    "                    if E.is_duplicated(t):\n",
    "                        continue\n",
    "                    \n",
    "                    pred_l = [0]*2\n",
    "                    cap_l = [0]*2\n",
    "                    incorr_l = [0]*2\n",
    "                    p_l = [0]*2\n",
    "                    B0_l = [0]*2\n",
    "                    points_l = [mpz(0)]*2\n",
    "                    \n",
    "                    # for the two new leaves, if they have not been visited, calculate their predictions, \n",
    "                    l1_sorted = tuple(sorted(l1))\n",
    "                    l2_sorted = tuple(sorted(l2))\n",
    "                    \n",
    "                    i_points = tree.points_cap[i]\n",
    "                    \n",
    "                    if l1_sorted not in leaves:\n",
    "                        leaves[l1_sorted] = CacheLeaf((l1,),x,y,i_points)\n",
    "                    \n",
    "                    Cache_l1 = leaves[l1_sorted]    \n",
    "                    pred_l[0], cap_l[0], incorr_l[0], p_l[0], B0_l[0], points_l[0] = Cache_l1.prediction, Cache_l1.num_captured, Cache_l1.num_captured_incorrect, Cache_l1.p, Cache_l1.B0, Cache_l1.points_cap\n",
    "                    \n",
    "                    if l2_sorted not in leaves:\n",
    "                        leaves[l2_sorted] = CacheLeaf((l2,),x,y,i_points)\n",
    "                    \n",
    "                    Cache_l2 = leaves[l2_sorted]\n",
    "                    pred_l[1], cap_l[1], incorr_l[1], p_l[1], B0_l[1], points_l[1] = Cache_l2.prediction, Cache_l2.num_captured, Cache_l2.num_captured_incorrect, Cache_l2.p, Cache_l2.B0, Cache_l2.points_cap\n",
    "                    \n",
    "                    # calculate the bounds for each leaves in the new tree\n",
    "                    loss_l1 = (incorr_l[0])/ndata\n",
    "                    loss_l2 = (incorr_l[1])/ndata\n",
    "                    loss_d0 = tree.p[i]*tree.num_captured[i]/ndata\n",
    "                    delta = loss_l1+loss_l2-loss_d0+lamb\n",
    "                    old_lbound = tree.lbound[:i]+tree.lbound[i+1:]\n",
    "                    new_lbound = [b+delta for b in old_lbound]+[tree.lbound[i]+loss_l2+lamb,tree.lbound[i]+loss_l1+lamb]\n",
    "                    \n",
    "                    #generate the new splitleaf for the new tree\n",
    "                    sl = generate_new_splitleaf(splitleaf_list, cap_l, incorr_l, ndata, t, lb, b0, lamb, R_c)\n",
    "                    #print(\"splitleaf_list, cap_l, incorr_l, ndata, t, lb, b0, lamb, R_c\",splitleaf_list, cap_l, incorr_l, ndata, t, lb, b0, lamb, R_c)\n",
    "                    #print('sl',sl)\n",
    "                    #construct the new tree\n",
    "                    tree_new = CacheTree(x = x, y = y, prefix = t,\n",
    "                                         #prediction = tree.prediction[:i]+tree.prediction[i+1:]+pred_l,\n",
    "                                         num_captured = tree.num_captured[:i]+tree.num_captured[i+1:]+cap_l,\n",
    "                                         #num_captured_incorrect = tree.num_captured_incorrect[:i]+tree.num_captured_incorrect[i+1:]+incorr_l,\n",
    "                                         deadleaf = tree.deadleaf[:i]+tree.deadleaf[i+1:]+[0,0],\n",
    "                                         splitleaf = sl,\n",
    "                                         lbound = new_lbound,\n",
    "                                         p = tree.p[:i]+tree.p[i+1:]+p_l,\n",
    "                                         B0 = tree.B0[:i]+tree.B0[i+1:]+B0_l,\n",
    "                                         lamb = lamb,\n",
    "                                         prior_metric=prior_metric,\n",
    "                                         points_cap = tree.points_cap[:i]+tree.points_cap[i+1:]+points_l\n",
    "                                        )\n",
    "\n",
    "                    #queue.append(tree_new)\n",
    "                    \"\"\"print(\"t:\",t)\n",
    "                    print(\"tree_new.num_captured:\",tree_new.num_captured)\n",
    "                    print(\"tree_new.deadleaf:\",tree_new.deadleaf)\n",
    "                    print(\"tree_new.splitleaf:\",tree_new.splitleaf)\n",
    "                    print(\"tree_new.p:\",tree_new.p)\n",
    "                    print(\"tree_new.B0:\", tree_new.B0)\"\"\"\n",
    "                    \n",
    "                    heapq.heappush(queue, (tree_new.metric,tree_new))\n",
    "                    R = tree_new.risk\n",
    "                    if R<R_c:\n",
    "                        d_c = tree_new\n",
    "                        R_c = R\n",
    "                    \n",
    "                    COUNT = COUNT+1\n",
    "                    \n",
    "                    #log(lines, lamb, tic, len(queue), d, tree_new, R, d_c, R_c)\n",
    "                    \n",
    "                   \n",
    "                \n",
    "    \"\"\"            \n",
    "    header = ['time', 'queue_size', 'split_tree', 'new_tree', 'new_tree_length', 'new_tree_objective',\n",
    "              'best_tree', 'best_tree_length', 'objective', 'lower_bound', 'accuracy', 'num_captured']\n",
    "    \n",
    "    fname = \"_\".join([str(nrule),str(ndata),prior_metric,str(lamb),\".txt\"])\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('%s\\n' % \";\".join(header))\n",
    "        f.write('\\n'.join(lines))\"\"\"\n",
    "\n",
    "\n",
    "    print(\"d_c\", d_c.prefix)\n",
    "    print(\"R_c\", R_c)\n",
    "    print(\"COUNT\", COUNT)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compas-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrule: 5\n",
      "ndata: 6907\n",
      "d_c ((-4,), (4, -5), (4, 5))\n",
      "R_c 0.3748675256985667\n",
      "COUNT 961365\n",
      "         78633100 function calls in 465.201 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-16-85107050ef81>:11(make_all_ones)\n",
      "      594    0.004    0.000    0.005    0.000 <ipython-input-16-85107050ef81>:23(rule_vand)\n",
      "      594    2.267    0.004    3.747    0.006 <ipython-input-17-94c5f54bcb5c>:9(rule_vectompz)\n",
      "   961365   58.817    0.000   95.102    0.000 <ipython-input-19-bf33e6033a7b>:45(<listcomp>)\n",
      "   961365   42.506    0.000   67.497    0.000 <ipython-input-19-bf33e6033a7b>:47(<listcomp>)\n",
      "   158231    0.392    0.000    0.392    0.000 <ipython-input-19-bf33e6033a7b>:58(__lt__)\n",
      "   961366   20.913    0.000  189.521    0.000 <ipython-input-19-bf33e6033a7b>:7(__init__)\n",
      "      198    0.019    0.000    3.773    0.019 <ipython-input-20-b16472b1b858>:7(__init__)\n",
      "  3204936   25.898    0.000   46.219    0.000 <ipython-input-21-1c738058ab5c>:12(is_duplicated)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-21-1c738058ab5c>:6(__init__)\n",
      "   943220    5.254    0.000    9.092    0.000 <ipython-input-21-1c738058ab5c>:9(eliminate)\n",
      "   961365   33.512    0.000   90.469    0.000 <ipython-input-23-989beb5a00dc>:1(generate_new_splitleaf)\n",
      "        1   91.537   91.537  465.034  465.034 <ipython-input-24-8d1821d54e65>:1(bbound)\n",
      "   961365    2.951    0.000    2.951    0.000 <ipython-input-24-8d1821d54e65>:138(<listcomp>)\n",
      "  1048606    3.661    0.000    3.661    0.000 <ipython-input-24-8d1821d54e65>:84(<listcomp>)\n",
      "        1    0.166    0.166  465.200  465.200 <string>:1(<module>)\n",
      "  2197165    7.375    0.000   23.353    0.000 _methods.py:31(_sum)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:43(_count_reduce_items)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:53(_mean)\n",
      "        4    0.000    0.000    0.000    0.000 enum.py:265(__call__)\n",
      "        4    0.000    0.000    0.000    0.000 enum.py:515(__new__)\n",
      "        2    0.000    0.000    0.000    0.000 enum.py:801(__and__)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1778(sum)\n",
      "        2    0.000    0.000    0.000    0.000 fromnumeric.py:2854(mean)\n",
      "       22    0.000    0.000    0.001    0.000 iostream.py:195(schedule)\n",
      "       20    0.000    0.000    0.000    0.000 iostream.py:300(_is_master_process)\n",
      "       20    0.000    0.000    0.000    0.000 iostream.py:313(_schedule_flush)\n",
      "       20    0.000    0.000    0.002    0.000 iostream.py:366(write)\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
      "        2    0.000    0.000    0.000    0.000 numeric.py:495(asanyarray)\n",
      "      594    0.007    0.000    1.434    0.002 re.py:184(sub)\n",
      "      594    0.003    0.000    0.004    0.000 re.py:286(_compile)\n",
      "       22    0.000    0.000    0.000    0.000 socket.py:333(send)\n",
      "        2    0.000    0.000    0.000    0.000 sre_compile.py:223(_compile_charset)\n",
      "        2    0.000    0.000    0.000    0.000 sre_compile.py:250(_optimize_charset)\n",
      "        2    0.000    0.000    0.000    0.000 sre_compile.py:376(_mk_bitmap)\n",
      "        2    0.000    0.000    0.000    0.000 sre_compile.py:378(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 sre_compile.py:414(_get_literal_prefix)\n",
      "        1    0.000    0.000    0.000    0.000 sre_compile.py:441(_get_charset_prefix)\n",
      "        1    0.000    0.000    0.000    0.000 sre_compile.py:482(_compile_info)\n",
      "        2    0.000    0.000    0.000    0.000 sre_compile.py:539(isstring)\n",
      "        1    0.000    0.000    0.000    0.000 sre_compile.py:542(_code)\n",
      "        1    0.000    0.000    0.001    0.001 sre_compile.py:557(compile)\n",
      "        1    0.000    0.000    0.000    0.000 sre_compile.py:64(_compile)\n",
      "        1    0.000    0.000    0.000    0.000 sre_parse.py:111(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 sre_parse.py:171(append)\n",
      "        1    0.000    0.000    0.000    0.000 sre_parse.py:173(getwidth)\n",
      "        1    0.000    0.000    0.000    0.000 sre_parse.py:223(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 sre_parse.py:232(__next)\n",
      "        7    0.000    0.000    0.000    0.000 sre_parse.py:248(match)\n",
      "        7    0.000    0.000    0.000    0.000 sre_parse.py:253(get)\n",
      "        2    0.000    0.000    0.000    0.000 sre_parse.py:285(tell)\n",
      "        3    0.000    0.000    0.000    0.000 sre_parse.py:294(_class_escape)\n",
      "        1    0.000    0.000    0.000    0.000 sre_parse.py:407(_parse_sub)\n",
      "        1    0.000    0.000    0.000    0.000 sre_parse.py:470(_parse)\n",
      "        1    0.000    0.000    0.000    0.000 sre_parse.py:76(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 sre_parse.py:81(groups)\n",
      "        1    0.000    0.000    0.000    0.000 sre_parse.py:828(fix_flags)\n",
      "        1    0.000    0.000    0.000    0.000 sre_parse.py:844(parse)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:1062(_wait_for_tstate_lock)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:1104(is_alive)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:506(is_set)\n",
      "   961366    3.940    0.000    4.268    0.000 {built-in method _heapq.heappop}\n",
      "   961366    3.289    0.000    3.352    0.000 {built-in method _heapq.heappush}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _sre.compile}\n",
      "      198    0.000    0.000    0.000    0.000 {built-in method builtins.abs}\n",
      "        1    0.000    0.000  465.201  465.201 {built-in method builtins.exec}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "       32    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      " 11178095   17.988    0.000   17.988    0.000 {built-in method builtins.len}\n",
      "   961369    2.681    0.000    2.681    0.000 {built-in method builtins.min}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.ord}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.pow}\n",
      "        5    0.000    0.000    0.002    0.000 {built-in method builtins.print}\n",
      "  6070886   23.959    0.000   23.959    0.000 {built-in method builtins.sorted}\n",
      " 13285796   31.111    0.000   31.111    0.000 {built-in method builtins.sum}\n",
      "   961960    2.371    0.000    2.371    0.000 {built-in method gmpy2.mpz}\n",
      "      594    0.001    0.000    0.001    0.000 {built-in method gmpy2.popcount}\n",
      " 19346460   36.285    0.000   36.285    0.000 {built-in method math.log2}\n",
      "   961565    7.995    0.000    7.995    0.000 {built-in method numpy.core.multiarray.array}\n",
      "       20    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "       22    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "  1133619    2.101    0.000    2.101    0.000 {method 'append' of 'list' objects}\n",
      "  1904585    3.500    0.000    3.500    0.000 {method 'copy' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'find' of 'bytearray' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "  3204936    5.509    0.000    5.509    0.000 {method 'keys' of 'dict' objects}\n",
      "   943220    2.135    0.000    2.135    0.000 {method 'pop' of 'list' objects}\n",
      "  2197167   15.978    0.000   15.978    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "      198    0.001    0.000    0.001    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "      594    1.424    0.002    1.424    0.002 {method 'sub' of '_sre.SRE_Pattern' objects}\n",
      "  2197164    9.647    0.000   33.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'translate' of 'bytearray' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run(\"bbound(x[:,:], y, lamb=0.01, prior_metric=\\\"entropy\\\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrule: 5\n",
      "ndata: 6907\n",
      "d_c ((-4,), (4, -5), (4, 5))\n",
      "R_c 0.3748675256985667\n",
      "COUNT 961709\n",
      "         59319226 function calls in 376.797 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-16-85107050ef81>:11(make_all_ones)\n",
      "      594    0.003    0.000    0.005    0.000 <ipython-input-16-85107050ef81>:23(rule_vand)\n",
      "      594    2.244    0.004    3.676    0.006 <ipython-input-17-94c5f54bcb5c>:9(rule_vectompz)\n",
      "   961709    7.256    0.000    7.256    0.000 <ipython-input-19-bf33e6033a7b>:51(<listcomp>)\n",
      "   961709   42.722    0.000   67.385    0.000 <ipython-input-19-bf33e6033a7b>:52(<listcomp>)\n",
      "   163704    0.405    0.000    0.405    0.000 <ipython-input-19-bf33e6033a7b>:58(__lt__)\n",
      "   961710   20.524    0.000  101.129    0.000 <ipython-input-19-bf33e6033a7b>:7(__init__)\n",
      "      198    0.019    0.000    3.701    0.019 <ipython-input-20-b16472b1b858>:7(__init__)\n",
      "  3206586   25.922    0.000   46.253    0.000 <ipython-input-21-1c738058ab5c>:12(is_duplicated)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-21-1c738058ab5c>:6(__init__)\n",
      "   943937    5.270    0.000    9.119    0.000 <ipython-input-21-1c738058ab5c>:9(eliminate)\n",
      "   961709   33.655    0.000   89.957    0.000 <ipython-input-23-989beb5a00dc>:1(generate_new_splitleaf)\n",
      "        1   92.212   92.212  376.629  376.629 <ipython-input-24-8d1821d54e65>:1(bbound)\n",
      "   961709    2.919    0.000    2.919    0.000 <ipython-input-24-8d1821d54e65>:138(<listcomp>)\n",
      "  1049106    3.621    0.000    3.621    0.000 <ipython-input-24-8d1821d54e65>:84(<listcomp>)\n",
      "        1    0.168    0.168  376.797  376.797 <string>:1(<module>)\n",
      "  2197840    7.309    0.000   22.851    0.000 _methods.py:31(_sum)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:43(_count_reduce_items)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:53(_mean)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1778(sum)\n",
      "        2    0.000    0.000    0.000    0.000 fromnumeric.py:2854(mean)\n",
      "       22    0.000    0.000    0.001    0.000 iostream.py:195(schedule)\n",
      "       20    0.000    0.000    0.000    0.000 iostream.py:300(_is_master_process)\n",
      "       20    0.000    0.000    0.000    0.000 iostream.py:313(_schedule_flush)\n",
      "       20    0.000    0.000    0.002    0.000 iostream.py:366(write)\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
      "        2    0.000    0.000    0.000    0.000 numeric.py:495(asanyarray)\n",
      "      594    0.006    0.000    1.391    0.002 re.py:184(sub)\n",
      "      594    0.003    0.000    0.003    0.000 re.py:286(_compile)\n",
      "       22    0.000    0.000    0.000    0.000 socket.py:333(send)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:1062(_wait_for_tstate_lock)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:1104(is_alive)\n",
      "       22    0.000    0.000    0.000    0.000 threading.py:506(is_set)\n",
      "   961710    3.974    0.000    4.318    0.000 {built-in method _heapq.heappop}\n",
      "   961710    3.294    0.000    3.354    0.000 {built-in method _heapq.heappush}\n",
      "      198    0.000    0.000    0.000    0.000 {built-in method builtins.abs}\n",
      "        1    0.000    0.000  376.797  376.797 {built-in method builtins.exec}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "       25    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      " 11184009   17.910    0.000   17.910    0.000 {built-in method builtins.len}\n",
      "   961710    2.637    0.000    2.637    0.000 {built-in method builtins.min}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.pow}\n",
      "        5    0.000    0.000    0.002    0.000 {built-in method builtins.print}\n",
      "  6073941   23.969    0.000   23.969    0.000 {built-in method builtins.sorted}\n",
      " 13291859   30.812    0.000   30.812    0.000 {built-in method builtins.sum}\n",
      "   962304    2.311    0.000    2.311    0.000 {built-in method gmpy2.mpz}\n",
      "      594    0.001    0.000    0.001    0.000 {built-in method gmpy2.popcount}\n",
      "   961909    7.765    0.000    7.765    0.000 {built-in method numpy.core.multiarray.array}\n",
      "       20    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "       22    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "  1134040    2.040    0.000    2.040    0.000 {method 'append' of 'list' objects}\n",
      "  1905646    3.466    0.000    3.466    0.000 {method 'copy' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "  3206586    5.525    0.000    5.525    0.000 {method 'keys' of 'dict' objects}\n",
      "   943937    2.108    0.000    2.108    0.000 {method 'pop' of 'list' objects}\n",
      "  2197842   15.541    0.000   15.541    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "      198    0.001    0.000    0.001    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "      594    1.382    0.002    1.382    0.002 {method 'sub' of '_sre.SRE_Pattern' objects}\n",
      "  2197839    9.800    0.000   32.651    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run(\"bbound(x[:,:], y, lamb=0.01, prior_metric=\\\"gini\\\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrule: 5\n",
      "ndata: 6907\n",
      "d_c ((5,), (-5, -4), (-5, 4))\n",
      "R_c 0.3748675256985667\n",
      "COUNT 1987208\n",
      "CPU times: user 3min 35s, sys: 680 ms, total: 3min 36s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#all data\n",
    "\n",
    "bbound(x, y, lamb=0.01, prior_metric=\"curiosity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrule: 5\n",
      "ndata: 6907\n",
      "d_c ((-4,), (4, -5), (4, 5))\n",
      "R_c 0.3748675256985667\n",
      "COUNT 1878542\n",
      "CPU times: user 2min 41s, sys: 444 ms, total: 2min 41s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#all data, 5 features\n",
    "\n",
    "bbound(x, y, lamb=0.01, prior_metric=\"bound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrule: 5\n",
      "ndata: 6907\n",
      "d_c ((-4,), (4, -5), (4, 5))\n",
      "R_c 0.3748675256985667\n",
      "COUNT 961365\n",
      "CPU times: user 1min 39s, sys: 64 ms, total: 1min 40s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#all data, 5 features\n",
    "\n",
    "bbound(x, y, lamb=0.01, prior_metric=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrule: 5\n",
      "ndata: 6907\n",
      "d_c ((-4,), (4, -5), (4, 5))\n",
      "R_c 0.3748675256985667\n",
      "COUNT 961709\n",
      "CPU times: user 1min 31s, sys: 52 ms, total: 1min 31s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#all data, 5 features\n",
    "\n",
    "bbound(x, y, lamb=0.01, prior_metric=\"gini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrule: 6\n",
      "ndata: 6907\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#all data, 5 features\n",
    "\n",
    "bbound(x6, y, lamb=0.01, prior_metric=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#all data, 5 features\n",
    "\n",
    "bbound(x6, y, lamb=0.01, prior_metric=\"gini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
