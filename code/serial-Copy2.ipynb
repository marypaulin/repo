{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An implementation of Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset\n",
    "df = pd.DataFrame(pd.read_csv('../data/compas-binary.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = df.as_matrix()[:,:13]\n",
    "\n",
    "y = df.as_matrix()[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association Rule Mining (Only one feature)\n",
    "\n",
    "#support\n",
    "#supp = [(x[:,i]*y).mean() for i in range(13)]\n",
    "#supp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3667168674698795,\n",
       " 0.592274678111588,\n",
       " 0.5300462249614792,\n",
       " 0.5184782608695652,\n",
       " 0.45773618016964024,\n",
       " 0.32459016393442625,\n",
       " 0.45069360675512665,\n",
       " 0.44644229291532195,\n",
       " 0.4233735747820255,\n",
       " 0.4932330827067669,\n",
       " 0.28986197049024276,\n",
       " 0.37864823348694315,\n",
       " 0.6614535418583257]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confidence\n",
    "conf1 = [sum(x_all[:,i]*y)/sum(x_all[:,i]) for i in range(13)]\n",
    "conf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48557089084065247,\n",
       " 0.4481314432989691,\n",
       " 0.45573665707893896,\n",
       " 0.45415065976281943,\n",
       " 0.4676032110091743,\n",
       " 0.4923509759099701,\n",
       " 0.7527272727272727,\n",
       " 0.7275,\n",
       " 0.711558854718982,\n",
       " 0.45544199390353235,\n",
       " 0.5382854764877236,\n",
       " 0.4822479928635147,\n",
       " 0.37143460807099093]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confidence\n",
    "conf0 = [sum((x_all[:,i]==0)*y)/sum((x_all[:,i]==0)) for i in range(13)]\n",
    "conf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBecause Using both conf1 and conf0 would select out too many features, \\nwhich is hard for the algorithm to run out,\\njust use conf1 to select out a small fraction of feature.\\n'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_idx = [conf1[i]>0.5 or conf0[i]>0.5 for i in range(len(conf1))]\n",
    "\"\"\"\n",
    "Because Using both conf1 and conf0 would select out too many features, \n",
    "which is hard for the algorithm to run out,\n",
    "just use conf1 to select out a small fraction of feature.\n",
    "\"\"\"\n",
    "#x_idx = [conf1[i]>0.5 for i in range(len(conf1))]\n",
    "#x_idx[0] = True # in the CORELS paper, gender is an important feature, so I add it manually\n",
    "#x_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select out these features\n",
    "#x = x_all[:,x_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#manaually select out 5 features, accoring to CORELS paper when lambda=0.01\n",
    "# sex:Female, age:18-20,age:21-22, juvenile-crimes:=0, priors:>3\n",
    "x_idx = [0,1,2,8,12]\n",
    "x = x_all[:,x_idx]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrule = x.shape[1]\n",
    "ndata = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate z, which is for the equivalent points bound\n",
    "z is the vector defined in algorithm 5 of the CORELS paper\n",
    "z is a binary vector indicating the data with a minority lable in its equivalent set\n",
    "\"\"\"\n",
    "z = pd.DataFrame([-1]*ndata).as_matrix()\n",
    "# enumerate through theses samples\n",
    "for i in range(ndata):\n",
    "    #if z[i,0]==-1, this sample i has not been put into its equivalent set\n",
    "    if z[i,0] == -1:\n",
    "        tag1 = np.array([True]*ndata)\n",
    "        for j in range(nrule):\n",
    "            rule_label = x[i][j]\n",
    "            #tag1 indicates which samples have exactly the same features with sample i\n",
    "            tag1 = (x[:,j] == rule_label)*tag1\n",
    "            \n",
    "        y_l = y[tag1]\n",
    "        pred = int(y_l.sum()/len(y_l) > 0.5)\n",
    "        #tag2 indicates the samples in a equiv set which have the minority label\n",
    "        tag2 = (y_l != pred)\n",
    "        z[tag1,0] = tag2\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul(antecedents, x, y, points_cap=tuple()):\n",
    "    \"\"\"\n",
    "    Function for calculating the predictions, number of data captured,\n",
    "    number of data incorrectly captured by the leaves, and b0 (defined in (28) in the CORELS paper).\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "    num_captured = []\n",
    "    num_captured_incorrect = []\n",
    "    p = []\n",
    "    B0 = [] # b0 is defined in (28)\n",
    "    points_captured = []\n",
    "    \n",
    "    num = len(antecedents)\n",
    "    \n",
    "    for i in range(num):\n",
    "        \n",
    "        \n",
    "        if (len(points_cap)>0):\n",
    "            #when split a leaf, points_cap is the vector indicating points captured by that leaf\n",
    "            tag = np.array(points_cap)\n",
    "            rule_index = abs(antecedents[i][-1])-1\n",
    "            rule_label = int(antecedents[i][-1]>0)\n",
    "            tag = (x[:,rule_index] == rule_label)*tag\n",
    "        else:\n",
    "            #when initialize the queue\n",
    "            rule_index = abs(antecedents[i][0])-1\n",
    "            rule_label = int(antecedents[i][0]>0)\n",
    "            tag = (x[:,rule_index] == rule_label)\n",
    "        \n",
    "        points_captured.append(tag)\n",
    "        \n",
    "        # the y's of these data captured by leaf antecedents[i]\n",
    "        y_leaf = y[tag]\n",
    "        \n",
    "        #b0 is defined in (28)\n",
    "        b0 = tag.dot(z)[0]/ndata\n",
    "        B0.append(b0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        num_cap = len(y_leaf)\n",
    "        num_captured.append(num_cap)\n",
    "        \n",
    "        if len(y_leaf)>0:\n",
    "            pred = int(y_leaf.sum()/len(y_leaf) > 0.5)\n",
    "            prediction.append(pred)\n",
    "            num_cap_incor = sum(y_leaf != pred)\n",
    "            num_captured_incorrect.append(num_cap_incor)\n",
    "            pr = num_cap_incor/num_cap\n",
    "            p.append(pr)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "            num_captured_incorrect.append(0)\n",
    "            p.append(0)\n",
    "\n",
    "    if num==1:\n",
    "        return prediction[0], num_captured[0], num_captured_incorrect[0], p[0], B0[0], points_captured[0]\n",
    "        \n",
    "    return prediction, num_captured, num_captured_incorrect, p, B0, points_captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheTree:\n",
    "    \"\"\"\n",
    "    A tree data structure.\n",
    "    prefix: a 2-d tuple to encode the leaves\n",
    "    prediction: a list to record the predictions of leaves\n",
    "    num_captured: a list to record number of data captured by the leaves\n",
    "    num_captured_incorrect: a list to record number of data incorrectly captured by the leaves\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, prefix,\n",
    "                 lamb, prior_metric = None,\n",
    "                 #prediction=None,\n",
    "                 num_captured=None,\n",
    "                 #num_captured_incorrect=None,\n",
    "                 deadleaf = None,\n",
    "                 splitleaf = None,\n",
    "                 lbound=None,\n",
    "                 p = None,\n",
    "                 B0 = None, points_cap = None):\n",
    "        self.prefix = prefix\n",
    "        #self.prediction = prediction\n",
    "        self.num_captured = num_captured\n",
    "        #self.num_captured_incorrect = num_captured_incorrect\n",
    "        self.p = p # the proportion of misclassified data in each leaf\n",
    "        self.deadleaf = deadleaf #a list indicate which leaves will never be split (support bound)\n",
    "        self.splitleaf = splitleaf #a queue of lists indicating which leaves will be split in next rounds (1 for split, 0 for not split)\n",
    "        self.lbound = lbound #a list of lower bound\n",
    "        self.B0 = B0 # a list of b0\n",
    "        self.points_cap = points_cap #a 2-d list (nleaves by ndata) indicating which points are captured by each leaf\n",
    "        \n",
    "        ndata = len(y)\n",
    "        l = len(prefix)\n",
    "        \n",
    "        self.risk = self.lbound[0]+(self.p[0]*self.num_captured[0])/ndata\n",
    "        \n",
    "        #print(self.prefix)\n",
    "        #print(self.lbound)\n",
    "        #print(self.splitleaf)\n",
    "        # which metrics to use for the priority queue\n",
    "        if (prior_metric==\"curiosity\"):\n",
    "            self.metric = min([self.lbound[i]/((ndata-self.num_captured[i])/len(y)) \n",
    "                                  for i in range(l) if self.splitleaf[0][i]==1])\n",
    "        elif (prior_metric==\"bound\"):\n",
    "            self.metric = min([self.lbound[i]\n",
    "                                  for i in range(l) if self.splitleaf[0][i]==1])\n",
    "        elif (prior_metric==\"entropy\"): \n",
    "            # entropy weighted by number of points captured\n",
    "            self.entropy = [(-self.p[i]*math.log2(self.p[i])-(1-self.p[i])*math.log2(1-self.p[i]))*self.num_captured[i] \n",
    "                            if self.p[i]!=0 and self.p[i]!=1 else 0 for i in range(l)]\n",
    "            self.metric = min([sum(self.entropy[:i]+self.entropy[i+1:])/(ndata-self.num_captured[i]) for i in range(l)])\n",
    "        elif (prior_metric==\"gini\"):\n",
    "            # gini index weighted by number of points captured\n",
    "            self.giniindex = [(2*self.p[i]*(1-self.p[i]))*self.num_captured[i] for i in range(l)]\n",
    "            self.metric = min([sum(self.giniindex[:i]+self.giniindex[i+1:])/(ndata-self.num_captured[i]) for i in range(l)])\n",
    "        else:\n",
    "            self.metric = 0\n",
    "\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        # define <, which will be used in the priority queue\n",
    "        return self.metric<other.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache every leaf\n",
    "\n",
    "class CacheLeaf:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, prediction=None,\n",
    "                 num_captured=None,\n",
    "                 num_captured_incorrect=None,\n",
    "                 p = None,\n",
    "                 B0 = None, \n",
    "                 points_cap = None):\n",
    "        self.prediction = prediction\n",
    "        self.num_captured = num_captured\n",
    "        self.num_captured_incorrect = num_captured_incorrect\n",
    "        self.p = p\n",
    "        self.B0 = B0 # a list of b0\n",
    "        self.points_cap = points_cap #a 2-d list (nleaves by ndata) indicating which points are captured by each leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eliminate:\n",
    "    \"\"\"\n",
    "    A data structure to record and identify\n",
    "    whether a tree has been visited/pruned\n",
    "    \"\"\"\n",
    "    def __init__(self, elim_dict = None):\n",
    "        self.elim_dict = {} # record these trees we have visited\n",
    "        \n",
    "    def eliminate(self, prefix):\n",
    "        self.elim_dict[tuple(sorted(prefix))] = 1\n",
    "        \n",
    "    def is_duplicated(self, prefix):\n",
    "        # if a tree is in the self.elim_dict, then we have already visited it\n",
    "        return tuple(sorted(prefix)) in self.elim_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def risk(tree,ndata):\n",
    "    return tree.lbound[0]+(tree.p[0]*tree.num_captured[0])/ndata\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(lines, lamb, tic, queue_size, prefix_old, tree_new, R, d_c, R_c):\n",
    "    \"log\"\n",
    "    t = tree_new.prefix\n",
    "    t_c = d_c.prefix\n",
    "    \n",
    "    the_time = str(time.time()-tic)\n",
    "    the_queue_size = str(queue_size)\n",
    "    the_split_tree = str(prefix_old)\n",
    "    the_new_tree = str(t)\n",
    "    the_new_tree_length = str(len(t))\n",
    "    the_new_tree_objective = str(R)\n",
    "    the_best_tree = str(t_c)\n",
    "    the_length = str(len(t_c))\n",
    "    the_obj = str(R_c)\n",
    "    the_lbound = str(d_c.lbound)\n",
    "    the_accuracy = str(1-(R_c - lamb*len(t_c)))\n",
    "    the_num_cap = str(d_c.num_captured)\n",
    "\n",
    "\n",
    "    line = \";\".join([the_time, the_queue_size, the_split_tree, \n",
    "                     the_new_tree, the_new_tree_length, the_new_tree_objective,\n",
    "                     the_best_tree, the_length, the_obj, \n",
    "                     the_lbound, the_accuracy, the_num_cap])\n",
    "    lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_splitleaf(splitleaf_list, cap_l, incorr_l, ndata, t, lb, b0, lamb, R_c):\n",
    "    \"\"\"\n",
    "    generate the new splitleaf for the new tree\n",
    "    \"\"\"\n",
    "    splitleaf_array = np.array(splitleaf_list)\n",
    "    sl = splitleaf_list.copy()\n",
    "\n",
    "    #(Lower bound on accurate antecedent support)\n",
    "    a_l = (sum(cap_l)-sum(incorr_l))/ndata - sum(cap_l)/ndata/2\n",
    "\n",
    "    #binary vector indicating split or not\n",
    "    splitleaf1 = [1]*(len(t)) #all leaves labeled as to be split\n",
    "    splitleaf2 = [0]*(len(t)-2)+[1,1] #l1,l2 labeled as to be split\n",
    "    splitleaf3 = [1]*(len(t)-2)+[0,0] #dp labeled as to be split\n",
    "\n",
    "    if lb+b0+lamb>=R_c or lb>=R_c:\n",
    "        # if equivalent points bound combined with the lookahead bound doesn't hold\n",
    "        # or if the hierarchical objective lower bound doesn't hold\n",
    "        # we need to split at least one leaf in dp\n",
    "\n",
    "        if a_l < lamb:\n",
    "        # if the bound doesn't hold, we need to split the leaf l1/l2 further\n",
    "\n",
    "            if len(splitleaf_list)>0:\n",
    "                split_l1_l2 = splitleaf_array[:,-1].sum()+splitleaf_array[:,-2].sum()\n",
    "\n",
    "                # if dp will have been split\n",
    "                if splitleaf_array.sum()-split_l1_l2>0:\n",
    "\n",
    "                    # if l1/l2 will have been split\n",
    "                    if split_l1_l2>0:\n",
    "                        sl.append(splitleaf1)\n",
    "\n",
    "                    # if l1/l2 will not have been split, we need to split l1/l2\n",
    "                    else:\n",
    "                        sl.append(splitleaf2)\n",
    "\n",
    "                # and we need to split leaves in dp, if dp will not have been split\n",
    "                else:\n",
    "\n",
    "                    # if l1/l2 will have been split\n",
    "                    if split_l1_l2>0:\n",
    "                        sl.append(splitleaf3)\n",
    "\n",
    "                    # if l1/l2 will not have been split, we need to split l1/l2\n",
    "                    else:\n",
    "                        sl.append(splitleaf2)\n",
    "                        sl.append(splitleaf3)\n",
    "            else:\n",
    "                sl.append(splitleaf2)\n",
    "                sl.append(splitleaf3)\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            if len(splitleaf_list)>0:\n",
    "                split_l1_l2 = splitleaf_array[:,-1].sum()+splitleaf_array[:,-2].sum()\n",
    "\n",
    "                # if dp will have been split\n",
    "                if splitleaf_array.sum()-split_l1_l2>0:\n",
    "                    sl.append(splitleaf1)\n",
    "\n",
    "                # and we need to split leaves in dp, if dp will not have been split\n",
    "                else:\n",
    "                    sl.append(splitleaf3)\n",
    "            else:\n",
    "                sl.append(splitleaf3)\n",
    "    else:\n",
    "\n",
    "        if a_l < lamb:\n",
    "            # if the bound doesn't hold, we need to split the leaf l1/l2 further\n",
    "\n",
    "\n",
    "            if len(splitleaf_list)>0:\n",
    "                split_l1_l2 = splitleaf_array[:,-1].sum()+splitleaf_array[:,-2].sum()\n",
    "\n",
    "                # if l1/l2 will have been split\n",
    "                if split_l1_l2>0:\n",
    "                    sl.append(splitleaf1)\n",
    "\n",
    "                # if l1/l2 will not have been split, we need to split l1/l2\n",
    "                else:\n",
    "                    sl.append(splitleaf2)\n",
    "            else:\n",
    "                sl.append(splitleaf2)\n",
    "\n",
    "        else:\n",
    "            sl.append(splitleaf1)\n",
    "        \n",
    "    return sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbound(x, y, lamb, prior_metric = None, MAXDEPTH = 4, niter=float('Inf')):\n",
    "    \"\"\"\n",
    "    An implementation of Algorithm\n",
    "    ## one copy of tree\n",
    "    ## mark which leaves to be split\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize best rule list and objective\n",
    "    d_c = None\n",
    "    R_c = 1\n",
    "\n",
    "    nrule = x.shape[1]\n",
    "    ndata = len(y)\n",
    "    print(\"nrule:\", nrule)\n",
    "    print(\"ndata:\", ndata)\n",
    "    \n",
    "    E = Eliminate()\n",
    "    tic = time.time()\n",
    "    \n",
    "    lines = [] # a list for log\n",
    "    leaves = {} # cache leaves\n",
    "\n",
    "    # initialize the queue to include all trees of just one split\n",
    "    queue = []\n",
    "    for r in range(1, nrule+1):\n",
    "        t = ((-r,),(r,))\n",
    "        pred_l, cap_l, incorr_l, p_l, B0_l, points_l= calcul(t, x, y)\n",
    "\n",
    "        leaves[(-r,)] = CacheLeaf(pred_l[0], cap_l[0], incorr_l[0], B0_l[0], points_l[0])\n",
    "        leaves[(r,)] = CacheLeaf(pred_l[1], cap_l[1], incorr_l[1], B0_l[1], points_l[1])\n",
    "        \n",
    "        lbound_l = [sum(incorr_l[:i]+incorr_l[i+1:])/ndata + lamb*2 for i in range(2)]\n",
    "        \n",
    "        tree0 = CacheTree(prefix = t, x = x, y = y, lamb=lamb, prior_metric=prior_metric, \n",
    "                          num_captured=cap_l, \n",
    "                          deadleaf = [0,0], splitleaf = [[1,1]], lbound=lbound_l,\n",
    "                          p = p_l, B0 = B0_l, points_cap = points_l)\n",
    "        heapq.heappush(queue, (tree0.metric,tree0))\n",
    "        #queue.append(tree0)\n",
    "        R = tree0.risk\n",
    "        if R<R_c:\n",
    "            d_c = tree0\n",
    "            R_c = R\n",
    "\n",
    "        #log(lines, lamb, tic, len(queue), tuple(), tree0, R, d_c, R_c) \n",
    "    \n",
    "    COUNT = nrule #count the total number of trees in the queue\n",
    "    while (queue) and COUNT<niter:\n",
    "        #tree = queue.pop(0)\n",
    "        (curio, tree)=heapq.heappop(queue)\n",
    "        d = tree.prefix\n",
    "        \n",
    "        \n",
    "        #print(\"=======COUNT=======\",COUNT)\n",
    "        #print(\"d\",d)\n",
    "        #print(\"R\",tree.lbound[0]+(tree.num_captured_incorrect[0])/len(y))\n",
    "        \n",
    "        # if we have visited this tree\n",
    "        if E.is_duplicated(d):\n",
    "            continue\n",
    "        else:\n",
    "            E.eliminate(d)\n",
    "        \n",
    "        # the leaves we are going to split\n",
    "        split_next = tree.splitleaf.copy()\n",
    "        spl = split_next.pop(0)\n",
    "        \n",
    "        # enumerate through all the leaves\n",
    "        for i in range(len(d)):\n",
    "            # if the leaf is dead, then continue\n",
    "            if tree.deadleaf[i]==1:\n",
    "                continue\n",
    "            \n",
    "            #(Lower bound on antecedent support)\n",
    "            # if this bound doesnot hold, set the leaf to be dead, and continue\n",
    "            if tree.num_captured[i]/ndata/2 < lamb:\n",
    "                tree.deadleaf[i] = 1\n",
    "                #print(\"==============dead==============\",i)\n",
    "                continue\n",
    "            \n",
    "            # 0 for not split; 1 for split\n",
    "            #if spl[0][i]==0:\n",
    "            if spl[i]==0:\n",
    "                continue\n",
    "\n",
    "            d0 = d[i] #d0 is the leaf we are going to split\n",
    "            dp = d[:i]+d[i+1:] #dp is the rest\n",
    "            \n",
    "            \n",
    "            # Restrict the depth of the tree\n",
    "            if len(d0)>=MAXDEPTH:\n",
    "                continue\n",
    "            \n",
    "            # we are going to split leaf i, and get 2 new leaves\n",
    "            # we will add the two new leaves to the end of the list\n",
    "            splitleaf_list = [split_next[k][:i]+split_next[k][i+1:]+split_next[k][i:i+1]*2\n",
    "                              for k in range(len(split_next))]\n",
    "            \n",
    "            \n",
    "            lb = tree.lbound[i] # the lower bound \n",
    "            b0 = tree.B0[i] # the b0 defined in (28) of the paper\n",
    "            \n",
    "            \n",
    "            \n",
    "            # split the leaf d0 with feature j\n",
    "            for j in range(1, nrule+1):\n",
    "                if (j not in d0)and(-j not in d0):\n",
    "                    # split leaf d0 with feature j, and get 2 leaves l1 and l2\n",
    "                    l1 = d0+(-j,)\n",
    "                    l2 = d0+(j,)\n",
    "                    t = dp+(l1, l2) # t is the new tree\n",
    "                    #print(\"t\",t)\n",
    "\n",
    "                    # if tree t is duplicated, continue\n",
    "                    if E.is_duplicated(t):\n",
    "                        continue\n",
    "                    \n",
    "                    pred_l = [0]*2\n",
    "                    cap_l = [0]*2\n",
    "                    incorr_l = [0]*2\n",
    "                    p_l = [0]*2\n",
    "                    B0_l = [0]*2\n",
    "                    points_l = [[0]*ndata]*2\n",
    "                    \n",
    "                    # for the two new leaves, if they have not been visited, calculate their predictions, \n",
    "                    l1_sorted = tuple(sorted(l1))\n",
    "                    l2_sorted = tuple(sorted(l2))\n",
    "                    Cache_l1 = leaves.get(l1_sorted, None)\n",
    "                    Cache_l2 = leaves.get(l2_sorted, None)\n",
    "                    \n",
    "                    i_points = tree.points_cap[i]\n",
    "                    \n",
    "                    \n",
    "                    if Cache_l1 != None:\n",
    "                        pred_l[0], cap_l[0], incorr_l[0], p_l[0], B0_l[0], points_l[0] = Cache_l1.prediction, Cache_l1.num_captured, Cache_l1.num_captured_incorrect, Cache_l1.p, Cache_l1.B0, Cache_l1.points_cap\n",
    "                    else:\n",
    "                        pred_l[0], cap_l[0], incorr_l[0], p_l[0], B0_l[0], points_l[0] = calcul((l1,),x,y,i_points)\n",
    "                        leaves[l1_sorted] = CacheLeaf(pred_l[0], cap_l[0], incorr_l[0], p_l[0], B0_l[0], points_l[0])\n",
    "                    \n",
    "                    if Cache_l2 != None:\n",
    "                        pred_l[1], cap_l[1], incorr_l[1], p_l[1], B0_l[1], points_l[1] = Cache_l2.prediction, Cache_l2.num_captured, Cache_l2.num_captured_incorrect, Cache_l2.p, Cache_l2.B0, Cache_l2.points_cap\n",
    "                    else:\n",
    "                        pred_l[1], cap_l[1], incorr_l[1], p_l[1], B0_l[1], points_l[1] = calcul((l2,),x,y,i_points)\n",
    "                        leaves[l2_sorted] = CacheLeaf(pred_l[1], cap_l[1], incorr_l[1], p_l[1], B0_l[1], points_l[1])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # calculate the bounds for each leaves in the new tree\n",
    "                    loss_l1 = (incorr_l[0])/ndata\n",
    "                    loss_l2 = (incorr_l[1])/ndata\n",
    "                    loss_d0 = tree.p[i]*tree.num_captured[i]/ndata\n",
    "                    delta = loss_l1+loss_l2-loss_d0+lamb\n",
    "                    old_lbound = tree.lbound[:i]+tree.lbound[i+1:]\n",
    "                    new_lbound = [b+delta for b in old_lbound]+[tree.lbound[i]+loss_l2+lamb,tree.lbound[i]+loss_l1+lamb]\n",
    "                    \n",
    "                    #generate the new splitleaf for the new tree\n",
    "                    sl = generate_new_splitleaf(splitleaf_list, cap_l, incorr_l, ndata, t, lb, b0, lamb, R_c)\n",
    "                    \n",
    "                    #construct the new tree\n",
    "                    tree_new = CacheTree(x = x, y = y, prefix = t,\n",
    "                                         #prediction = tree.prediction[:i]+tree.prediction[i+1:]+pred_l,\n",
    "                                         num_captured = tree.num_captured[:i]+tree.num_captured[i+1:]+cap_l,\n",
    "                                         #num_captured_incorrect = tree.num_captured_incorrect[:i]+tree.num_captured_incorrect[i+1:]+incorr_l,\n",
    "                                         deadleaf = tree.deadleaf[:i]+tree.deadleaf[i+1:]+[0,0],\n",
    "                                         splitleaf = sl,\n",
    "                                         lbound = new_lbound,\n",
    "                                         p = tree.p[:i]+tree.p[i+1:]+p_l,\n",
    "                                         B0 = tree.B0[:i]+tree.B0[i+1:]+B0_l,\n",
    "                                         lamb = lamb,\n",
    "                                         prior_metric=prior_metric,\n",
    "                                         points_cap = tree.points_cap[:i]+tree.points_cap[i+1:]+points_l\n",
    "                                        )\n",
    "\n",
    "                    #queue.append(tree_new)\n",
    "                    heapq.heappush(queue, (tree_new.metric,tree_new))\n",
    "                    R = tree_new.risk\n",
    "                    if R<R_c:\n",
    "                        d_c = tree_new\n",
    "                        R_c = R\n",
    "                    \n",
    "                    COUNT = COUNT+1\n",
    "                    \n",
    "                    #log(lines, lamb, tic, len(queue), d, tree_new, R, d_c, R_c)\n",
    "                    \n",
    "                   \n",
    "                \n",
    "    \"\"\"            \n",
    "    header = ['time', 'queue_size', 'split_tree', 'new_tree', 'new_tree_length', 'new_tree_objective',\n",
    "              'best_tree', 'best_tree_length', 'objective', 'lower_bound', 'accuracy', 'num_captured']\n",
    "    \n",
    "    fname = \"_\".join([str(nrule),str(ndata),prior_metric,str(lamb),\".txt\"])\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('%s\\n' % \";\".join(header))\n",
    "        f.write('\\n'.join(lines))\"\"\"\n",
    "\n",
    "\n",
    "    print(\"d_c\", d_c.prefix)\n",
    "    print(\"R_c\", R_c)\n",
    "    print(\"COUNT\", COUNT)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compas-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run(\"bbound(x[:,:], y, lamb=0.01, prior_metric=\\\"curiosity\\\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run(\"bbound(x[:,:], y, lamb=0.01, prior_metric=\\\"gini\\\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#all data\n",
    "\n",
    "bbound(x, y, lamb=0.01, prior_metric=\"curiosity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#all data, 5 features\n",
    "\n",
    "bbound(x, y, lamb=0.01, prior_metric=\"bound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrule: 5\n",
      "ndata: 6907\n",
      "d_c ((-4,), (4, -5), (4, 5))\n",
      "R_c 0.3748675256985667\n",
      "COUNT 961365\n",
      "CPU times: user 2min 30s, sys: 252 ms, total: 2min 30s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#all data, 5 features\n",
    "\n",
    "bbound(x, y, lamb=0.01, prior_metric=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrule: 5\n",
      "ndata: 6907\n",
      "d_c ((-4,), (4, -5), (4, 5))\n",
      "R_c 0.3748675256985667\n",
      "COUNT 961709\n",
      "CPU times: user 2min 13s, sys: 296 ms, total: 2min 14s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#all data, 5 features\n",
    "\n",
    "bbound(x, y, lamb=0.01, prior_metric=\"gini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
