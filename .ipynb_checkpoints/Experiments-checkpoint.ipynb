{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from lib.experiments.accuracy import plot_accuracy_analysis\n",
    " \n",
    "dataset_name = 'compas-binary'\n",
    "plot_accuracy_analysis(dataset_name, 'Test Accuracy vs Tree Width ({})'.format(dataset_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalability Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from lib.data_structures.dataset import read_dataframe\n",
    "from lib.experiments.scalability import plot_scalability_analysis\n",
    "\n",
    "dataset_name = 'compas-binary'\n",
    "timeout = 60\n",
    "\n",
    "# dataset_name = 'census_c1s5ky0.2_0_0_test'\n",
    "# timeout = 20\n",
    "\n",
    "# Scalablity Analysis for CART\n",
    "dataset = read_dataframe('data/scalability/{}/{}.csv'.format(dataset_name, 'cart'))\n",
    "plot_scalability_analysis(dataset, 'CART Scalability', z_limit=timeout)\n",
    "\n",
    "# Scalablity Analysis for OSDT\n",
    "dataset = read_dataframe('data/scalability/{}/{}.csv'.format(dataset_name, 'osdt'))\n",
    "plot_scalability_analysis(dataset, 'OSDT Scalability', z_limit=timeout)\n",
    "\n",
    "# Scalablity Analysis for Parallel OSDT\n",
    "# for core_count in [1, 2, 4, 8, 16, 32, 60]:\n",
    "for core_count in [1, 2]:\n",
    "    dataset = read_dataframe('data/scalability/{}/parallel_osdt_{}_core.csv'.format(dataset_name, core_count))\n",
    "    plot_scalability_analysis(dataset, 'Parallel OSDT Scalability ({} Clients, 1 Server)'.format(core_count), z_limit=timeout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from lib.data_structures.dataset import read_dataframe\n",
    "\n",
    "directory = 'data/convergence'\n",
    "paths = [\n",
    "    join(directory, name)\n",
    "    for name in listdir(directory)\n",
    "    if isfile(join(directory, name))]\n",
    "\n",
    "for path in paths:\n",
    "    dataset = read_dataframe(path)\n",
    "    (n, m) = dataset.shape\n",
    "\n",
    "    x = dataset.values[:,0]\n",
    "    y1 = dataset.values[:,1]\n",
    "    y2 = dataset.values[:,2]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 2), dpi=100)\n",
    "    plt.plot(x, y1, label='lowerbound', markersize=2, marker='o', linewidth=1)\n",
    "    plt.plot(x, y2, label='upperbound', markersize=2, marker='o', linewidth=1)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Objective Bounds')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title('Objective Bounds vs Time ({})'.format(basename(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work Distribution Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join, basename\n",
    "\n",
    "from lib.data_structures.dataset import read_dataframe\n",
    "\n",
    "directory = 'data/distribution'\n",
    "paths = [\n",
    "    join(directory, name)\n",
    "    for name in listdir(directory)\n",
    "    if isfile(join(directory, name))]\n",
    "\n",
    "for path in paths:\n",
    "    dataset = read_dataframe(path)\n",
    "    (n, m) = dataset.shape\n",
    "\n",
    "    x = dataset.values[:,0]\n",
    "    y1 = dataset.values[:,1]\n",
    "    y2 = dataset.values[:,2]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 2), dpi=100)\n",
    "    plt.plot(x, y1, label='local queue', markersize=2, marker='o', linewidth=1)\n",
    "    plt.plot(x, y2, label='global queue', markersize=2, marker='o', linewidth=1)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Queue Length')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title('Queue Length vs Time ({})'.format(basename(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join, basename\n",
    "\n",
    "from lib.data_structures.dataset import read_dataframe\n",
    "\n",
    "directory = 'data/memory'\n",
    "paths = [\n",
    "    join(directory, name)\n",
    "    for name in listdir(directory)\n",
    "    if isfile(join(directory, name))]\n",
    "\n",
    "for path in paths:\n",
    "    dataset = read_dataframe(path)\n",
    "    (n, m) = dataset.shape\n",
    "\n",
    "    x = dataset.values[:,0]\n",
    "    y = dataset.values[:,1]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 2), dpi=100)\n",
    "    plt.plot(x, y, markersize=2, marker='o', linewidth=1)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Entries Memoized')\n",
    "    plt.grid()\n",
    "    plt.title('Memoization Table Size vs Time ({})'.format(basename(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "# local imports\n",
    "from lib.models.parallel_osdt_classifier import ParallelOSDTClassifier\n",
    "from lib.data_structures.dataset import read_dataframe\n",
    "\n",
    "# Using COMPAS as an example\n",
    "dataset = read_dataframe('data/preprocessed/census.csv') \n",
    "(n, m) = dataset.shape\n",
    "X = dataset.values[:n,:-1]\n",
    "y = dataset.values[:n,-1]\n",
    "\n",
    "hyperparameters = {\n",
    "    'regularization': 0.2, # Regularization coefficient which effects the penalty on model complexity\n",
    "\n",
    "    'max_depth': float('Inf'), # User-specified limit on the model\n",
    "    'max_time': float('Inf'), # User-specified limit on the runtime \n",
    "\n",
    "    'workers': 1, # Parameter that varies based on how much computational resource is available\n",
    "\n",
    "    'visualize_model': False, # Toggle whether a rule-list visualization is rendered\n",
    "    'visualize_training': False,  # Toggle whether a dependency graph is streamed at runtime\n",
    "    'verbose': False, # Toggle whether event messages are printed\n",
    "    'log': False, # Toggle whether client processes log to logs/work_<id>.log files\n",
    "    \n",
    "    'configuration': { # More configurations around toggling optimizations and prioritization options\n",
    "        'priority_metric': 'uncertainty', # Decides how tasks are prioritized\n",
    "        'deprioritization': 0.01, # Decides how much to push back a task if it has pending dependencies\n",
    "\n",
    "        # Note that Leaf Permutation Bound (Theorem 6) is \n",
    "        # Toggles the assumption about objective independence when composing subtrees (Theorem 1)\n",
    "        # Disabling this actually breaks convergence due to information loss\n",
    "        'hierarchical_lowerbound': True, \n",
    "        # Toggles whether problems are pruned based on insufficient accuracy (compared to other results) (Lemma 2)\n",
    "        'look_ahead': True,\n",
    "        # Toggles whether a split is avoided based on insufficient support (proxy for accuracy gain) (Theorem 3)\n",
    "        'support_lowerbound': True,\n",
    "        # Toggles whether a split is avoided based on insufficient potential accuracy gain (Theorem 4)\n",
    "        'incremental_accuracy_lowerbound': True,\n",
    "        # Toggles whether a problem is pruned based on insufficient accuracy (in general) (Theorem 5)\n",
    "        'accuracy_lowerbound': True,\n",
    "        # Toggles whether problem equivalence is based solely on the capture set (Similar to Corollary 6)\n",
    "        'capture_equivalence': True,\n",
    "        # Hamming distance used to propagate bounding information of similar problems (Theorem 7 + some more...)\n",
    "        \"similarity_threshold\": 5,\n",
    "        # Toggles whether equivalent points contribute to the lowerbound (Proposition 8 and Theorem 9)\n",
    "        'equivalent_point_lowerbound': True,\n",
    "\n",
    "        # Toggles compression of dataset based on equivalent point aggregation\n",
    "        'equivalent_point_compression': False,\n",
    "        # Toggles whether asynchronous tasks can be cancelled after being issued\n",
    "        'task_cancellation': True,\n",
    "        # Toggles whether look_ahead prunes using objective upperbounds (This builds on top of look_ahead)\n",
    "        'interval_look_ahead': True,\n",
    "        # Cooldown timer (seconds) on synchornization operations\n",
    "        'synchronization_cooldown': 0.01\n",
    "    }\n",
    "}\n",
    "\n",
    "model = ParallelOSDTClassifier(**hyperparameters)\n",
    "model.fit(X, y)\n",
    "cProfile.run('model.fit(X, y)', sort='tottime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third-party imports\n",
    "import cProfile\n",
    "from time import time\n",
    "\n",
    "# local imports\n",
    "from lib.models.parallel_osdt_classifier import ParallelOSDTClassifier\n",
    "from lib.data_structures.dataset import read_dataframe\n",
    "\n",
    "# Using COMPAS as an example\n",
    "dataset = read_dataframe('data/preprocessed/iris.csv') \n",
    "(n, m) = dataset.shape\n",
    "print(n, m)\n",
    "\n",
    "m = 58 # Use only sepal features\n",
    "\n",
    "X = dataset.values[:n,:m]\n",
    "y = dataset.values[:n,-3] # Select first vs other for label\n",
    "\n",
    "hyperparameters = {\n",
    "    'regularization': 0.01, # Regularization coefficient which effects the penalty on model complexity\n",
    "\n",
    "    'max_depth': float('Inf'), # User-specified limit on the model\n",
    "    'max_time': float('Inf'), # User-specified limit on the runtime \n",
    "\n",
    "    'workers': 1, # Parameter that varies based on how much computational resource is available\n",
    "\n",
    "    'visualize_model': True, # Toggle whether a rule-list visualization is rendered\n",
    "    'visualize_training': False,  # Toggle whether a dependency graph is streamed at runtime\n",
    "    'verbose': False, # Toggle whether event messages are printed\n",
    "    'log': False, # Toggle whether client processes log to logs/work_<id>.log files\n",
    "    'profile': False, # Toggle Snapshots for Profiling Memory Usage\n",
    "    \n",
    "    'configuration': { # More configurations around toggling optimizations and prioritization options\n",
    "        'priority_metric': 'uniform', # Decides how tasks are prioritized\n",
    "        'deprioritization': 0.1, # Decides how much to push back a task if it has pending dependencies\n",
    "\n",
    "        # Note that Leaf Permutation Bound (Theorem 6) is \n",
    "        # Toggles the assumption about objective independence when composing subtrees (Theorem 1)\n",
    "        # Disabling this actually breaks convergence due to information loss\n",
    "        'hierarchical_lowerbound': True, \n",
    "        # Toggles whether problems are pruned based on insufficient accuracy (compared to other results) (Lemma 2)\n",
    "        'look_ahead': True,\n",
    "        # Toggles whether a split is avoided based on insufficient support (proxy for accuracy gain) (Theorem 3)\n",
    "        'support_lowerbound': True,\n",
    "        # Toggles whether a split is avoided based on insufficient potential accuracy gain (Theorem 4)\n",
    "        'incremental_accuracy_lowerbound': True,\n",
    "        # Toggles whether a problem is pruned based on insufficient accuracy (in general) (Theorem 5)\n",
    "        'accuracy_lowerbound': True,\n",
    "        # Toggles whether problem equivalence is based solely on the capture set (Similar to Corollary 6)\n",
    "        'capture_equivalence': True,\n",
    "        # Hamming distance used to propagate bounding information of similar problems (Theorem 7 + some more...)\n",
    "        \"similarity_threshold\": float('Inf'),\n",
    "        # Toggles whether equivalent points contribute to the lowerbound (Proposition 8 and Theorem 9)\n",
    "        'equivalent_point_lowerbound': True,\n",
    "\n",
    "        # Toggles compression of dataset based on equivalent point aggregation\n",
    "        'equivalent_point_compression': True,\n",
    "        # Toggles whether asynchronous tasks can be cancelled after being issued\n",
    "        'task_cancellation': True,\n",
    "        # Toggles whether look_ahead prunes using objective upperbounds (This builds on top of look_ahead)\n",
    "        'interval_look_ahead': True,\n",
    "        # Cooldown timer (seconds) on synchornization operations\n",
    "        'synchronization_cooldown': 0.1,\n",
    "        # Cache Limit\n",
    "        'independence': 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "# none = 42690\n",
    "# low  = 42690\n",
    "# medium = 42690\n",
    "# high = 42690\n",
    "\n",
    "# accuracy = 98%\n",
    "\n",
    "start = time()\n",
    "model = ParallelOSDTClassifier(**hyperparameters)\n",
    "model.fit(X, y)\n",
    "# cProfile.run('model.fit(X, y)', sort='tottime')\n",
    "prediction = model.predict(X)\n",
    "prediction = prediction.reshape(1, n)\n",
    "print('Runtime: {} Seconds'.format(time() - start))\n",
    "print('Prediction: \\n{}'.format(prediction))\n",
    "print('Training Accuracy: {}'.format(model.score(X, y)))\n",
    "print('Visualization: \\n{}'.format(model.model.visualization))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\" \n",
    "\tDesc: \t1. sepal-length\n",
    "\t\t\t2. sepal-width\n",
    "\t\t\t3. petal-length\n",
    "\t\t\t4. petal-width\n",
    "\t\t\t5. classification \n",
    "\t\t\t\t- Iris-setosa\n",
    "\t\t\t\t- Iris-versicolor\n",
    "\t\t\t\t- Iris-virginica\n",
    "\"\"\"\n",
    "\n",
    "# Plot Truth\n",
    "\n",
    "dataset = read_dataframe('data/iris/iris.csv')\n",
    "(n,m) = dataset.shape\n",
    "lengths = dataset.values[:,:-1]\n",
    "flower_type = dataset.values[:,-1]\n",
    "\n",
    "for i in range(n):\n",
    "\tx, y = [lengths[i,0], lengths[i,1]]\n",
    "\tscale = 100.0\n",
    "\t# determine color\n",
    "\tflower = flower_type[i]\n",
    "\tcolor = \"\"\n",
    "\tif flower == \"Iris-setosa\":\n",
    "\t\tcolor = \"red\"\n",
    "\telif flower == \"Iris-versicolor\":\n",
    "\t\tcolor = \"blue\"\n",
    "\telif flower == \"Iris-virginica\":\n",
    "\t\tcolor = \"blue\"\n",
    "\n",
    "\tplt.scatter(x, y, s=scale, c=color, alpha=1, edgecolor=\"none\")\n",
    "\n",
    "# Legend\n",
    "red_patch = mpatches.Patch(color='red', label='iris setosa')\n",
    "green_patch = mpatches.Patch(color='blue', label='iris versicolor')\n",
    "blue_patch = mpatches.Patch(color='blue', label='iris virginica')\n",
    "plt.legend(handles=[red_patch, green_patch, blue_patch])\n",
    "\n",
    "plt.title(\"The Iris Data Set (Ground Truth)\", fontsize=18)\n",
    "plt.xlabel('petal length', fontsize=15)\n",
    "plt.ylabel('petal width', fontsize=15)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot Predictions\n",
    "\n",
    "for i in range(n):\n",
    "\tx, y = [lengths[i,0], lengths[i,1]]\n",
    "\tscale = 100.0\n",
    "\t# determine color\n",
    "\tprediction[i]\n",
    "\tcolor = \"\"\n",
    "\tif prediction[i,0] == 0:\n",
    "\t\tcolor = \"blue\"\n",
    "\telif prediction[i,0] == 1:\n",
    "\t\tcolor = \"red\"\n",
    "\n",
    "\tplt.scatter(x, y, s=scale, c=color, alpha=1, edgecolor=\"none\")\n",
    "\n",
    "# Legend\n",
    "red_patch = mpatches.Patch(color='red', label='iris setosa')\n",
    "green_patch = mpatches.Patch(color='blue', label='iris versicolor')\n",
    "blue_patch = mpatches.Patch(color='blue', label='iris virginica')\n",
    "plt.legend(handles=[red_patch, green_patch, blue_patch])\n",
    "\n",
    "plt.title(\"The Iris Data Set (Predictions)\", fontsize=18)\n",
    "plt.xlabel('petal length', fontsize=15)\n",
    "plt.ylabel('petal width', fontsize=15)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "length_domain = np.arange(4,8,0.1)\n",
    "width_domain = np.arange(2,4.5,0.1)\n",
    "\n",
    "X = []\n",
    "\n",
    "for l in length_domain:\n",
    "    for w in width_domain:\n",
    "        row = [0,0,l,w]\n",
    "        X.append(row)\n",
    "\n",
    "y = x.reshape(-1, 1)\n",
    "h = x * y\n",
    "print(h)\n",
    "\n",
    "cs = plt.contourf(h, levels=[10, 30, 50],\n",
    "    colors=['#808080', '#A0A0A0', '#C0C0C0'], extend='both')\n",
    "cs.cmap.set_over('red')\n",
    "cs.cmap.set_under('blue')\n",
    "cs.changed()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
