{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Below are various preprocessing procedures for each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third-party imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Dataframe Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# votes = pd.DataFrame(pd.read_csv('voting-records/house-votes-84.data',header=None))\n",
    "# votes[votes==\"?\"] = np.nan\n",
    "# votes=votes.dropna(axis = 0)\n",
    "# votes[votes==\"y\"] = 1\n",
    "# votes[votes==\"n\"] = 0\n",
    "# votes[votes==\"democrat\"] = 1\n",
    "# votes[votes==\"republican\"] = 0\n",
    "# votes.columns=[\"democrat\", \"handicapped-infants\", \"water-project-cost-sharing\", \"adoption-of-the-budget-resolution\", \n",
    "#                \"physician-fee-freeze\", \"el-salvador-aid\", \"religious-groups-in-schools\", \"anti-satellite-test-ban\",\n",
    "#                \"aid-to-nicaraguan-contras\", \"mx-missile\", \"immigration\", \"synfuels-corporation-cutback\", \n",
    "#                \"education-spending\", \"superfund-right-to-sue\", \"crime\", \"duty-free-exports\", \n",
    "#                \"export-administration-act-south-africa\"\n",
    "#               ]\n",
    "# votes.to_csv('voting-records-binary.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 5\n",
      "['sepal_length>=4.3', 'sepal_length>=4.4', 'sepal_length>=4.5', 'sepal_length>=4.6', 'sepal_length>=4.7', 'sepal_length>=4.8', 'sepal_length>=4.9', 'sepal_length>=5.0', 'sepal_length>=5.1', 'sepal_length>=5.2', 'sepal_length>=5.3', 'sepal_length>=5.4', 'sepal_length>=5.5', 'sepal_length>=5.6', 'sepal_length>=5.7', 'sepal_length>=5.8', 'sepal_length>=5.9', 'sepal_length>=6.0', 'sepal_length>=6.1', 'sepal_length>=6.2', 'sepal_length>=6.3', 'sepal_length>=6.4', 'sepal_length>=6.5', 'sepal_length>=6.6', 'sepal_length>=6.7', 'sepal_length>=6.8', 'sepal_length>=6.9', 'sepal_length>=7.0', 'sepal_length>=7.1', 'sepal_length>=7.2', 'sepal_length>=7.3', 'sepal_length>=7.4', 'sepal_length>=7.6', 'sepal_length>=7.7', 'sepal_length>=7.9', 'sepal_width>=2.0', 'sepal_width>=2.2', 'sepal_width>=2.3', 'sepal_width>=2.4', 'sepal_width>=2.5', 'sepal_width>=2.6', 'sepal_width>=2.7', 'sepal_width>=2.8', 'sepal_width>=2.9', 'sepal_width>=3.0', 'sepal_width>=3.1', 'sepal_width>=3.2', 'sepal_width>=3.3', 'sepal_width>=3.4', 'sepal_width>=3.5', 'sepal_width>=3.6', 'sepal_width>=3.7', 'sepal_width>=3.8', 'sepal_width>=3.9', 'sepal_width>=4.0', 'sepal_width>=4.1', 'sepal_width>=4.2', 'sepal_width>=4.4', 'petal_length>=1.0', 'petal_length>=1.1', 'petal_length>=1.2', 'petal_length>=1.3', 'petal_length>=1.4', 'petal_length>=1.5', 'petal_length>=1.6', 'petal_length>=1.7', 'petal_length>=1.9', 'petal_length>=3.0', 'petal_length>=3.3', 'petal_length>=3.5', 'petal_length>=3.6', 'petal_length>=3.7', 'petal_length>=3.8', 'petal_length>=3.9', 'petal_length>=4.0', 'petal_length>=4.1', 'petal_length>=4.2', 'petal_length>=4.3', 'petal_length>=4.4', 'petal_length>=4.5', 'petal_length>=4.6', 'petal_length>=4.7', 'petal_length>=4.8', 'petal_length>=4.9', 'petal_length>=5.0', 'petal_length>=5.1', 'petal_length>=5.2', 'petal_length>=5.3', 'petal_length>=5.4', 'petal_length>=5.5', 'petal_length>=5.6', 'petal_length>=5.7', 'petal_length>=5.8', 'petal_length>=5.9', 'petal_length>=6.0', 'petal_length>=6.1', 'petal_length>=6.3', 'petal_length>=6.4', 'petal_length>=6.6', 'petal_length>=6.7', 'petal_length>=6.9', 'petal_width>=0.1', 'petal_width>=0.2', 'petal_width>=0.3', 'petal_width>=0.4', 'petal_width>=0.5', 'petal_width>=0.6', 'petal_width>=1.0', 'petal_width>=1.1', 'petal_width>=1.2', 'petal_width>=1.3', 'petal_width>=1.4', 'petal_width>=1.5', 'petal_width>=1.6', 'petal_width>=1.7', 'petal_width>=1.8', 'petal_width>=1.9', 'petal_width>=2.0', 'petal_width>=2.1', 'petal_width>=2.2', 'petal_width>=2.3', 'petal_width>=2.4', 'petal_width>=2.5', 'class=Iris-setosa', 'class=Iris-versicolor', 'class=Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# monk1\n",
    "dataset = pd.DataFrame(pd.read_csv('data/iris/iris.csv', delimiter=\",\"))\n",
    "feature_values = tuple(sorted(set(dataset[column])) for column in dataset.columns)\n",
    "(N, M) = dataset.shape\n",
    "headers = []\n",
    "for k, feature in enumerate(dataset.columns):\n",
    "    if type(feature_values[k][0]) in {int, float}:\n",
    "        for value in feature_values[k]:\n",
    "            headers.append('{}>={}'.format(feature, value))\n",
    "    else:\n",
    "        for value in feature_values[k]:\n",
    "            headers.append('{}={}'.format(feature, value))\n",
    "            \n",
    "rows = [headers]\n",
    "print(headers)\n",
    " # remove semicolon to reveal the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monk1\n",
    "monk1 = pd.DataFrame(pd.read_csv('data/monks-problems/monks-1.train',header=None,delimiter=\" \"))\n",
    "monk1 = monk1.drop(columns=[0,8])\n",
    "monk1 = monk1.applymap(str)\n",
    "monk1 = pd.get_dummies(monk1, prefix=[\"class\", \"a1\", \"a2\", \"a3\", \"a4\", \"a5\", \"a6\"])\n",
    "monk1 = monk1.drop(columns=['class_0', 'a1_3', 'a2_3', 'a3_2', 'a4_3', 'a5_4', 'a6_2'])\n",
    "columns = [\"a1_1\",\"a1_2\",\"a2_1\",\"a2_2\",\"a3_1\",\"a4_1\",\"a4_2\",\"a5_1\",\"a5_2\",\"a5_3\",\"a6_1\",\"class_1\"]\n",
    "monk1 = monk1[columns]\n",
    "\n",
    "monk1.to_csv('data/preprocessed/monk1-train.csv',index=False,sep=';')\n",
    "monk1; # remove semicolon to reveal the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monk2\n",
    "monk2 = pd.DataFrame(pd.read_csv('data/monks-problems/monks-2.train',header=None,delimiter=\" \"))\n",
    "monk2 = monk2.drop(columns=[0,8])\n",
    "monk2 = monk2.applymap(str)\n",
    "monk2 = pd.get_dummies(monk2, prefix=[\"class\", \"a1\", \"a2\", \"a3\", \"a4\", \"a5\", \"a6\"])\n",
    "monk2 = monk2.drop(columns=['class_0', 'a1_3', 'a2_3', 'a3_2', 'a4_3', 'a5_4', 'a6_2'])\n",
    "columns = [\"a1_1\",\"a1_2\",\"a2_1\",\"a2_2\",\"a3_1\",\"a4_1\",\"a4_2\",\"a5_1\",\"a5_2\",\"a5_3\",\"a6_1\",\"class_1\"]\n",
    "monk2 = monk2[columns]\n",
    "\n",
    "monk2.to_csv('data/preprocessed/monk2-train.csv',index=False,sep=';')\n",
    "monk2; # remove semicolon to reveal the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monk 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monk3\n",
    "monk3 = pd.DataFrame(pd.read_csv('data/monks-problems/monks-3.train',header=None,delimiter=\" \"))\n",
    "monk3 = monk3.drop(columns=[0,8])\n",
    "monk3 = monk3.applymap(str)\n",
    "monk3 = pd.get_dummies(monk3, prefix=[\"class\", \"a1\", \"a2\", \"a3\", \"a4\", \"a5\", \"a6\"])\n",
    "monk3 = monk3.drop(columns=['class_0', 'a1_3', 'a2_3', 'a3_2', 'a4_3', 'a5_4', 'a6_2'])\n",
    "columns = [\"a1_1\",\"a1_2\",\"a2_1\",\"a2_2\",\"a3_1\",\"a4_1\",\"a4_2\",\"a5_1\",\"a5_2\",\"a5_3\",\"a6_1\",\"class_1\"]\n",
    "monk3 = monk3[columns]\n",
    "\n",
    "monk3.to_csv('data/preprocessed/monk3-train.csv',index=False,sep=';')\n",
    "monk3; # remove semicolon to reveal the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_scale = pd.DataFrame(pd.read_csv('data/balance-scale/balance-scale.data',header=None,delimiter=\",\"))\n",
    "balance_scale.columns = [\"class\", \"a1\", \"a2\", \"a3\", \"a4\"]\n",
    "balance_scale['class'] = (balance_scale['class']==\"B\")\n",
    "balance_scale\n",
    "\n",
    "balance_scale = balance_scale.applymap(str)\n",
    "balance_scale = pd.get_dummies(balance_scale, prefix=[\"class\", \"a1\", \"a2\", \"a3\", \"a4\"])\n",
    "balance_scale['class']=balance_scale['class_True']\n",
    "balance_scale = balance_scale.drop(columns=['class_True','class_False', 'a1_5', 'a2_5','a3_5','a4_5'])\n",
    "\n",
    "balance_scale.to_csv('data/preprocessed/balance-scale.csv',index=False,sep=';')\n",
    "balance_scale; # remove semicolon to reveal the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = pd.DataFrame(pd.read_csv('data/car/car.data',header=None,delimiter=\",\"))\n",
    "car.columns = [\"a1\", \"a2\", \"a3\", \"a4\", \"a5\", \"a6\", \"class\"]\n",
    "car['class'] = 1-(car['class']==\"unacc\")\n",
    "car = car.applymap(str)\n",
    "car = pd.get_dummies(car)\n",
    "car = car.drop(columns=['a1_vhigh', 'a2_vhigh', 'a3_5more', 'a4_more', 'a5_small', 'a6_med', 'class_0'])\n",
    "car.to_csv('data/preprocessed/car-evaluation.csv',index=False,sep=';')\n",
    "car; # remove semicolon to reveal the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPAS is already in a compatible format\n",
    "compas = pd.DataFrame(pd.read_csv('data/compas/compas-binary.csv'))\n",
    "compas['age:<21']=compas['age:18-20']\n",
    "compas['age:<23']=compas['age:18-20']+compas['age:21-22']\n",
    "compas['age:<26']=compas['age:18-20']+compas['age:21-22']+compas['age:23-25']\n",
    "compas['age:<46']=compas['age:18-20']+compas['age:21-22']+compas['age:23-25']+compas['age:26-45']\n",
    "\n",
    "compas = compas.drop(columns=['age:18-20', 'age:21-22', 'age:23-25', 'age:26-45', 'age:>45'])\n",
    "\n",
    "columns = ['sex:Female','age:<21','age:<23','age:<26', 'age:<46', \n",
    "           'juvenile-felonies:=0', 'juvenile-misdemeanors:=0', 'juvenile-crimes:=0',\n",
    "           'priors:=0', 'priors:=1', 'priors:2-3', 'priors:>3', 'recidivate-within-two-years:1']\n",
    "\n",
    "compas = compas[columns]\n",
    "compas\n",
    "compas.to_csv('data/preprocessed/compas-binary.csv',index=False,sep=';')\n",
    "\n",
    "compas; # remove semicolon to reveal the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I see, I don't think it's necessary to store the partitioned data copies on disk\n",
    "# We now have a library function to do crss validation in memory so the stuff below can go away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold # import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10,random_state=2018,shuffle=True) # Define the split - into 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "compas = pd.DataFrame(pd.read_csv('data/preprocessed/compas-binary.csv',sep=\";\"))\n",
    "monk1 = pd.DataFrame(pd.read_csv('data/preprocessed/monk1-train.csv',sep=\";\"))\n",
    "monk2 = pd.DataFrame(pd.read_csv('data/preprocessed/monk2-train.csv',sep=\";\"))\n",
    "monk3 = pd.DataFrame(pd.read_csv('data/preprocessed/monk3-train.csv',sep=\";\"))\n",
    "balance = pd.DataFrame(pd.read_csv('data/preprocessed/balance-scale.csv',sep=\";\"))\n",
    "tictactoe = pd.DataFrame(pd.read_csv('data/preprocessed/tic-tac-toe.csv',sep=\";\"))\n",
    "car = pd.DataFrame(pd.read_csv('data/preprocessed/car-evaluation.csv',sep=\";\"))\n",
    "fico = pd.DataFrame(pd.read_csv('data/preprocessed/fico_binary.csv',sep=\";\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for train_index, test_index in kf.split(compas):\n",
    "    i=i+1\n",
    "    compas_train = compas.iloc[train_index]\n",
    "    compas_test = compas.iloc[test_index]\n",
    "    \n",
    "    compas_train.to_csv('preprocessed/compas-binary.csv.train'+str(i)+'.csv',index=False,sep=';')\n",
    "    compas_test.to_csv('preprocessed/compas-binary.csv.test'+str(i)+'.csv',index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for train_index, test_index in kf.split(monk1):\n",
    "    i=i+1\n",
    "    monk1_train = monk1.iloc[train_index]\n",
    "    monk1_test = monk1.iloc[test_index]\n",
    "    \n",
    "    monk1_train.to_csv('preprocessed/monk1-train.csv.train'+str(i)+'.csv',index=False,sep=';')\n",
    "    monk1_test.to_csv('preprocessed/monk1-train.csv.test'+str(i)+'.csv',index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for train_index, test_index in kf.split(monk2):\n",
    "    i=i+1\n",
    "    monk2_train = monk2.iloc[train_index]\n",
    "    monk2_test = monk2.iloc[test_index]\n",
    "    \n",
    "    monk2_train.to_csv('preprocessed/monk2-train.csv.train'+str(i)+'.csv',index=False,sep=';')\n",
    "    monk2_test.to_csv('preprocessed/monk2-train.csv.test'+str(i)+'.csv',index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for train_index, test_index in kf.split(monk3):\n",
    "    i=i+1\n",
    "    monk3_train = monk3.iloc[train_index]\n",
    "    monk3_test = monk3.iloc[test_index]\n",
    "    \n",
    "    monk3_train.to_csv('preprocessed/monk3-train.csv.train'+str(i)+'.csv',index=False,sep=';')\n",
    "    monk3_test.to_csv('preprocessed/monk3-train.csv.test'+str(i)+'.csv',index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for train_index, test_index in kf.split(balance):\n",
    "    i=i+1\n",
    "    balance_train = balance.iloc[train_index]\n",
    "    balance_test = balance.iloc[test_index]\n",
    "    \n",
    "    balance_train.to_csv('preprocessed/balance-scale.csv.train'+str(i)+'.csv',index=False,sep=';')\n",
    "    balance_test.to_csv('preprocessed/balance-scale.csv.test'+str(i)+'.csv',index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for train_index, test_index in kf.split(tictactoe):\n",
    "    i=i+1\n",
    "    tictactoe_train = tictactoe.iloc[train_index]\n",
    "    tictactoe_test = tictactoe.iloc[test_index]\n",
    "    \n",
    "    tictactoe_train.to_csv('preprocessed/tic-tac-toe.csv.train'+str(i)+'.csv',index=False,sep=';')\n",
    "    tictactoe_test.to_csv('preprocessed/tic-tac-toe.csv.test'+str(i)+'.csv',index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for train_index, test_index in kf.split(car):\n",
    "    i=i+1\n",
    "    car_train = car.iloc[train_index]\n",
    "    car_test = car.iloc[test_index]\n",
    "    \n",
    "    car_train.to_csv('preprocessed/car-evaluation.csv.train'+str(i)+'.csv',index=False,sep=';')\n",
    "    car_test.to_csv('preprocessed/car-evaluation.csv.test'+str(i)+'.csv',index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for train_index, test_index in kf.split(fico):\n",
    "    i=i+1\n",
    "    fico_train = fico.iloc[train_index]\n",
    "    fico_test = fico.iloc[test_index]\n",
    "    \n",
    "    fico_train.to_csv('preprocessed/fico_binary.csv.train'+str(i)+'.csv',index=False,sep=';')\n",
    "    fico_test.to_csv('preprocessed/fico_binary.csv.test'+str(i)+'.csv',index=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
